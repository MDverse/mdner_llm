{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de66c95",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0eca9c",
   "metadata": {},
   "source": [
    "# Test the LLM annotation with PyDantic and Instructor  ‚òë\n",
    "\n",
    "### ‚ö†Ô∏è Background  \n",
    "Previous attempts used **XML tags** (e.g., `<MOL>POPC</MOL>`) to annotate entities in Molecular Dynamics (MD) texts.  \n",
    "However, this approach had several issues:  \n",
    "- LLMs sometimes **modified the input text**  \n",
    "- The **XML structure** was often **broken** (missing tags)  \n",
    "- Parsing results were **fragile and inconsistent**\n",
    "\n",
    "\n",
    "### üéØ Objectives  \n",
    "- ‚úÖ **Improve** LLM annotation outputs for MD text  \n",
    "- üßæ **Enforce structured JSON** outputs instead of XML  \n",
    "- üîç **Validate** and **control schema** using:\n",
    "  - **Pydantic** ‚Üí defines the expected JSON structure  \n",
    "  - **Instructor** ‚Üí forces the LLM to comply with that structure\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57520f",
   "metadata": {},
   "source": [
    "## Package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --packages numpy,pandas,matplotlib,pydantic,openai,groq,instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63878c74",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1100f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import instructor\n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Import utility functions and constants\n",
    "from utils import annotate, check_json_validity, PROMPT, visualize_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_GROQ = \"llama-3.1-8b-instant\"\n",
    "MODEL_OPENAI = \"o3-mini-2025-01-31\"\n",
    "\n",
    "EX_MD_TEXTS = [\n",
    "    \"The protein was simulated using AMBER with the TIP3P water model.\",\n",
    "    \"Visualization was done with VMD, and the MD run used GROMOS96 parameters.\",\n",
    "    \"A POPC:CHOL membrane was built using CHARMM-GUI.\",\n",
    "    \"\"\"Improved Coarse-Grained Modeling of Cholesterol-Containing Lipid Bilayers\n",
    "Cholesterol trafficking, which is an essential function in mammalian cells, is intimately connected to molecular-scale interactions through cholesterol modulation of membrane structure and dynamics and interaction with membrane receptors. Since these effects of cholesterol occur on micro- to millisecond time scales, it is essential to develop accurate coarse-grained simulation models that can reach these time scales. Cholesterol has been shown experimentally to thicken the membrane and increase phospholipid tail order between 0 and 40% cholesterol, above which these effects plateau or slightly decrease. Here, we showed that the published MARTINI coarse-grained force-field for phospholipid (POPC) and cholesterol fails to capture these effects. Using reference atomistic simulations, we systematically modified POPC and cholesterol bonded parameters in MARTINI to improve its performance. We showed that the corrections to pseudobond angles between glycerol and the lipid tails and around the oleoyl double bond particle (the angle-corrected model ) slightly improves the agreement of MARTINI with experimentally measured thermal, elastic, and dynamic properties of POPC membranes. The angle-corrected model improves prediction of the thickening and ordering effects up to 40% cholesterol but overestimates these effects at higher cholesterol concentration. In accordance with prior work that showed the cholesterol rough face methyl groups are important for limiting cholesterol self-association, we revised the coarse-grained representation of these methyl groups to better match cholesterol-cholesterol radial distribution functions from atomistic simulations. In addition, by using a finer-grained representation of the branched cholesterol tail than MARTINI, we improved predictions of lipid tail order and bilayer thickness across a wide range of concentrations. Finally, transferability testing shows that a model incorporating our revised parameters into DOPC outperforms other CG models in a DOPC/cholesterol simulation series, which further argues for its efficacy and generalizability. These results argue for the importance of systematic optimization for coarse-graining biologically important molecules like cholesterol with complicated molecular structure.\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7222b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq client (https://python.useinstructor.com/integrations/groq/)\n",
    "client_groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "client_groq = instructor.from_provider(f\"groq/{MODEL_GROQ}\", mode=instructor.Mode.JSON)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client_openai = instructor.from_provider(f\"openai/{MODEL_OPENAI}\", mode=instructor.Mode.JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c4647",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a527e",
   "metadata": {},
   "source": [
    "# I. Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9f597",
   "metadata": {},
   "source": [
    "## I.1 Without validation üö´\n",
    "\n",
    "The LLM output is returned **as-is**, without enforcing a schema.  \n",
    "This helps check if the model produces **valid JSON** and adheres to the expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a85d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[0]\n",
    "response = annotate(texts_to_annotate, MODEL_GROQ, client=client_groq, validation=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìù INPUT TEXT \")\n",
    "print(\"=\" * 80)\n",
    "print(texts_to_annotate)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"ü§ñ MODEL RESPONSE ({MODEL_GROQ})\")\n",
    "print(\"=\" * 80)\n",
    "print(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "# test if the response is a valid JSON\n",
    "if check_json_validity(response):\n",
    "    print(\"Valid JSON ‚úÖ\")\n",
    "else:\n",
    "    print(\"Invalid JSON ‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3dd20",
   "metadata": {},
   "source": [
    "## 2. With validation ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad2a2e",
   "metadata": {},
   "source": [
    "Now, we test it with pydantic and instructor !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[0]\n",
    "response = annotate(texts_to_annotate, MODEL_OPENAI, client=client_openai, validation=True)\n",
    "\n",
    "print(f\"Input:\\n{texts_to_annotate}\\n\")\n",
    "print(f\"Response with validation ({MODEL_OPENAI}):\\n{response}\")\n",
    "# test if the response is a valid JSON\n",
    "if check_json_validity(response):\n",
    "    print(\"Valid JSON ‚úÖ\")\n",
    "else:\n",
    "    print(\"Invalid JSON ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0936332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entities using displacy\n",
    "visualize_entities(texts_to_annotate, response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c395c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[0]\n",
    "response = annotate(texts_to_annotate, MODEL_GROQ, client=client_groq, validation=True)\n",
    "\n",
    "print(f\"Input:\\n{texts_to_annotate}\\n\")\n",
    "print(f\"Response with validation ({MODEL_GROQ}):\\n{response}\")\n",
    "# test if the response is a valid JSON\n",
    "if check_json_validity(response):\n",
    "    print(\"Valid JSON ‚úÖ\")\n",
    "else:\n",
    "    print(\"Invalid JSON ‚ùå\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681fc08",
   "metadata": {},
   "source": [
    "# II. Example from our annotation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90e59c",
   "metadata": {},
   "source": [
    "Now we want to validate our observations statistically. We‚Äôll query the model 100 times and compare the results obtained with and without format validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c389981",
   "metadata": {},
   "source": [
    "## II.1 Without validation üö´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5906889",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[3]\n",
    "stat_num_iterations = 100\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "entities_list = []\n",
    "\n",
    "for _ in tqdm(range(stat_num_iterations), desc=\"Running annotations\"):\n",
    "    entities_list.append(annotate(texts_to_annotate, MODEL_GROQ, client_groq, validation=False))\n",
    "    if check_json_validity(entities_list[-1]):\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìù Input text:\\n{texts_to_annotate}\\n\")\n",
    "print(f\"üìä Summary after {stat_num_iterations} runs with {MODEL_GROQ}:\")\n",
    "print(f\"‚úî Valid responses:   {valid_count}\")\n",
    "print(f\"‚ùå Invalid responses: {invalid_count}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b58298",
   "metadata": {},
   "source": [
    "## II.2 With validation ‚úÖ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760148d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[3]\n",
    "stat_num_iterations = 3\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "entities_validated_list = []\n",
    "\n",
    "for _ in tqdm(range(stat_num_iterations), desc=\"Running annotations\"):\n",
    "    entities_validated_list.append(annotate(texts_to_annotate, MODEL_GROQ, client_groq, validation=True))\n",
    "    #time.sleep(30)  # To avoid rate limiting\n",
    "    if check_json_validity(entities_validated_list[-1]):\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìù Input text:\\n{texts_to_annotate}\\n\")\n",
    "print(f\"üìä Summary after {stat_num_iterations} runs with {MODEL_GROQ}:\")\n",
    "print(f\"‚úî Valid responses:   {valid_count}\")\n",
    "print(f\"‚ùå Invalid responses: {invalid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96903cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_validated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e7927",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_validated_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccaa01",
   "metadata": {},
   "source": [
    "# III. OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[3]\n",
    "stat_num_iterations = 100\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "for _ in tqdm(range(stat_num_iterations), desc=\"Running annotations\"):\n",
    "    response = annotate(texts_to_annotate, MODEL_OPENAI, client_openai, validation=False)\n",
    "    if check_json_validity(response):\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìù Input text:\\n{texts_to_annotate}\\n\")\n",
    "print(f\"üìä Summary after {stat_num_iterations} runs with {MODEL_OPENAI}:\")\n",
    "print(f\"‚úî Valid responses:   {valid_count}\")\n",
    "print(f\"‚ùå Invalid responses: {invalid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_annotate = EX_MD_TEXTS[3]\n",
    "stat_num_iterations = 100\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "for _ in tqdm(range(stat_num_iterations), desc=\"Running annotations\"):\n",
    "    response = annotate(texts_to_annotate, MODEL_OPENAI, client_openai, validation=True)\n",
    "    if check_json_validity(response):\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìù Input text:\\n{texts_to_annotate}\\n\")\n",
    "print(f\"üìä Summary after {stat_num_iterations} runs with {MODEL_OPENAI}:\")\n",
    "print(f\"‚úî Valid responses:   {valid_count}\")\n",
    "print(f\"‚ùå Invalid responses: {invalid_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e603b6",
   "metadata": {},
   "source": [
    "# Conclusion üí°\n",
    "\n",
    "The conclusion of the tests is that validation helps ensure the output follows the expected schema. \n",
    "\n",
    "But we notice that with OpenAI models, the JSON is often already well-structured. In contrast, smaller models like llama-3.1-8b-instant tend to produce less consistent outputs. The strong performance of larger models is also partly due to effective prompt engineering techniques, such as few-shot examples.\n",
    "\n",
    "\n",
    "In short, combining good prompt design with schema validation with Pydantic and Instructor provide the structure and validation needed to ensure models generate consistent and trustworthy outputs üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdner-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
