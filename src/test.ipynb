{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b560b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "# First API call with reasoning\n",
    "response = client.chat.completions.create(\n",
    "  model=\"moonshotai/kimi-k2-thinking\",\n",
    "  messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many r's are in the word 'strawberry'?\"\n",
    "          }\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Extract the assistant message with reasoning_details\n",
    "response = response.choices[0].message\n",
    "\n",
    "# Preserve the assistant message with reasoning_details\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"How many r's are in the word 'strawberry'?\"},\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.content,\n",
    "    \"reasoning_details\": response.reasoning_details  # Pass back unmodified\n",
    "  },\n",
    "  {\"role\": \"user\", \"content\": \"Are you sure? Think carefully.\"}\n",
    "]\n",
    "\n",
    "# Second API call - model continues reasoning from where it left off\n",
    "response2 = client.chat.completions.create(\n",
    "  model=\"moonshotai/kimi-k2-thinking\",\n",
    "  messages=messages,\n",
    "  extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e88751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-1763992616-0Y6484v6MtVkotj5sSgO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='You are right to ask me to double-check — it\\'s a simple question that can be easy to miscount if you rush.\\n\\nLet me break it down **very carefully**:\\n\\nThe word is **s-t-r-a-w-b-e-r-r-y**.\\n\\nI\\'ll go letter by letter and mark each \\'r\\':\\n1. s\\n2. t\\n3. **r** ← (first r)\\n4. a\\n5. w\\n6. b\\n7. e\\n8. **r** ← (second r)\\n9. **r** ← (third r)\\n10. y\\n\\nCounting them: **1, 2, 3**.\\n\\nThere are **3** r\\'s in \"strawberry\".', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='The user is asking me to reconsider my answer to a very simple question. They want me to think carefully about how many r\\'s are in \"strawberry\".\\n\\nLet me break down the word letter by letter:\\n\\nS - T - R - A - W - B - E - R - R - Y\\n\\nLet me index them:\\n1. S\\n2. T\\n3. R\\n4. A\\n5. W\\n6. B\\n7. E\\n8. R\\n9. R\\n10. Y\\n\\nSo the r\\'s are at positions 3, 8, and 9. That\\'s 3 r\\'s.\\n\\nWait, let me double-check. The word is \"strawberry\". Let me spell it out slowly:\\ns-t-r-a-w-b-e-r-r-y\\n\\nLet me count the r\\'s again:\\n- The first \\'r\\' is the 3rd letter: s-t-**r**-a-w-b-e-r-r-y\\n- The second \\'r\\' is the 8th letter: s-t-r-a-w-b-e-**r**-r-y\\n- The third \\'r\\' is the 9th letter: s-t-r-a-w-b-e-r-**r**-y\\n\\nSo I count three r\\'s. Let me verify by writing the word and circling the r\\'s:\\nstrawberry\\n--r------  (first r)\\n--------r- (second r)\\n---------r (third r)\\n\\nYes, there are three r\\'s. \\n\\nBut the user is prompting me to \"think carefully\" which suggests they might be trying to trick me or they think there\\'s a nuance I\\'m missing. Let me think about possible pitfalls:\\n\\n1. **Case sensitivity**: The word is lowercase \"strawberry\" - case doesn\\'t change the count.\\n2. **Hidden characters**: There\\'s no hidden text or formatting in the word itself.\\n3. **Phonetic interpretation**: Some might think about pronunciation, but the question is about spelling, not sounds.\\n4. **Linguistic nuance**: Could there be something about the etymology or alternative spellings? No, standard English spelling is \"strawberry\".\\n5. **The word \"strawberry\" itself**: Is there something about the structure? \"Straw\" + \"berry\"? The word \"berry\" has two r\\'s, and \"straw\" contributes one r. So 1 + 2 = 3.\\n6. **Double letters**: The word has a double-r (\"rr\"), but those are still two distinct r\\'s.\\n\\nLet me count one more time, very carefully:\\ns (1)\\nt (2) \\nr (3) ← first r\\na (4)\\nw (5)\\nb (6)\\ne (7)\\nr (8) ← second r\\nr (9) ← third r\\ny (10)\\n\\nThree r\\'s. The answer is definitively 3.\\n\\nThe user\\'s tone might be:\\n- Testing my ability to handle simple questions\\n- Trying to see if I overthink simple problems\\n- Referencing the common internet meme/trope about AI systems (particularly earlier language models) having difficulty counting letters in words\\n- Just being playful or skeptical\\n\\nI should be confident in my answer but acknowledge their request for careful thinking. I\\'ll provide a clear, methodical explanation that shows I\\'ve actually thought through it, not just given a quick answer.\\n\\nLet me construct a response that:\\n1. Confirms the answer is 3\\n2. Shows the breakdown visually\\n3. Addresses the \"think carefully\" prompt directly\\n4. Perhaps mentions why they might be asking me to double-check (the AI counting meme)\\n\\nI want to be helpful but not condescending. The question is simple, but the prompt suggests they want me to demonstrate reasoning. I\\'ll provide a thorough but not overly elaborate explanation.\\n\\nFinal check: s-t-r-a-w-b-e-r-r-y. Yes, three r\\'s. I\\'m absolutely certain.', reasoning_details=[{'format': 'unknown', 'index': 0, 'type': 'reasoning.text', 'text': 'The user is asking me to reconsider my answer to a very simple question. They want me to think carefully about how many r\\'s are in \"strawberry\".\\n\\nLet me break down the word letter by letter:\\n\\nS - T - R - A - W - B - E - R - R - Y\\n\\nLet me index them:\\n1. S\\n2. T\\n3. R\\n4. A\\n5. W\\n6. B\\n7. E\\n8. R\\n9. R\\n10. Y\\n\\nSo the r\\'s are at positions 3, 8, and 9. That\\'s 3 r\\'s.\\n\\nWait, let me double-check. The word is \"strawberry\". Let me spell it out slowly:\\ns-t-r-a-w-b-e-r-r-y\\n\\nLet me count the r\\'s again:\\n- The first \\'r\\' is the 3rd letter: s-t-**r**-a-w-b-e-r-r-y\\n- The second \\'r\\' is the 8th letter: s-t-r-a-w-b-e-**r**-r-y\\n- The third \\'r\\' is the 9th letter: s-t-r-a-w-b-e-r-**r**-y\\n\\nSo I count three r\\'s. Let me verify by writing the word and circling the r\\'s:\\nstrawberry\\n--r------  (first r)\\n--------r- (second r)\\n---------r (third r)\\n\\nYes, there are three r\\'s. \\n\\nBut the user is prompting me to \"think carefully\" which suggests they might be trying to trick me or they think there\\'s a nuance I\\'m missing. Let me think about possible pitfalls:\\n\\n1. **Case sensitivity**: The word is lowercase \"strawberry\" - case doesn\\'t change the count.\\n2. **Hidden characters**: There\\'s no hidden text or formatting in the word itself.\\n3. **Phonetic interpretation**: Some might think about pronunciation, but the question is about spelling, not sounds.\\n4. **Linguistic nuance**: Could there be something about the etymology or alternative spellings? No, standard English spelling is \"strawberry\".\\n5. **The word \"strawberry\" itself**: Is there something about the structure? \"Straw\" + \"berry\"? The word \"berry\" has two r\\'s, and \"straw\" contributes one r. So 1 + 2 = 3.\\n6. **Double letters**: The word has a double-r (\"rr\"), but those are still two distinct r\\'s.\\n\\nLet me count one more time, very carefully:\\ns (1)\\nt (2) \\nr (3) ← first r\\na (4)\\nw (5)\\nb (6)\\ne (7)\\nr (8) ← second r\\nr (9) ← third r\\ny (10)\\n\\nThree r\\'s. The answer is definitively 3.\\n\\nThe user\\'s tone might be:\\n- Testing my ability to handle simple questions\\n- Trying to see if I overthink simple problems\\n- Referencing the common internet meme/trope about AI systems (particularly earlier language models) having difficulty counting letters in words\\n- Just being playful or skeptical\\n\\nI should be confident in my answer but acknowledge their request for careful thinking. I\\'ll provide a clear, methodical explanation that shows I\\'ve actually thought through it, not just given a quick answer.\\n\\nLet me construct a response that:\\n1. Confirms the answer is 3\\n2. Shows the breakdown visually\\n3. Addresses the \"think carefully\" prompt directly\\n4. Perhaps mentions why they might be asking me to double-check (the AI counting meme)\\n\\nI want to be helpful but not condescending. The question is simple, but the prompt suggests they want me to demonstrate reasoning. I\\'ll provide a thorough but not overly elaborate explanation.\\n\\nFinal check: s-t-r-a-w-b-e-r-r-y. Yes, three r\\'s. I\\'m absolutely certain.'}]), native_finish_reason='stop')], created=1763992616, model='moonshotai/kimi-k2-thinking', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=958, prompt_tokens=82, total_tokens=1040, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=21)), provider='Fireworks')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5673436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=' There are **3** r\\'s in the word \"strawberry\".', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='  The user is asking a simple counting question: \"How many r\\'s are in the word \\'strawberry\\'?\"\\n\\nLet me spell out the word and count the \\'r\\'s:\\n\\nS-T-R-A-W-B-E-R-R-Y\\n\\nLet me go letter by letter:\\n1. S\\n2. T\\n3. R (that\\'s 1 r)\\n4. A\\n5. W\\n6. B\\n7. E\\n8. R (that\\'s 2 r\\'s)\\n9. R (that\\'s 3 r\\'s)\\n10. Y\\n\\nSo there are 3 r\\'s in the word \"strawberry\".\\n\\nI should answer this directly and clearly. ', reasoning_details=[{'format': 'unknown', 'index': 0, 'type': 'reasoning.text', 'text': '  The user is asking a simple counting question: \"How many r\\'s are in the word \\'strawberry\\'?\"\\n\\nLet me spell out the word and count the \\'r\\'s:\\n\\nS-T-R-A-W-B-E-R-R-Y\\n\\nLet me go letter by letter:\\n1. S\\n2. T\\n3. R (that\\'s 1 r)\\n4. A\\n5. W\\n6. B\\n7. E\\n8. R (that\\'s 2 r\\'s)\\n9. R (that\\'s 3 r\\'s)\\n10. Y\\n\\nSo there are 3 r\\'s in the word \"strawberry\".\\n\\nI should answer this directly and clearly. '}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "836ab18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5668e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/touami/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple, Union, List\n",
    "\n",
    "import instructor\n",
    "from instructor.core.client import Instructor\n",
    "from instructor.core import InstructorRetryException, ValidationError as InstructorValidationError\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from dotenv import load_dotenv\n",
    "from openai.types.chat import ChatCompletion\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.openrouter import OpenRouterProvider\n",
    "from pydantic_ai.exceptions import UnexpectedModelBehavior\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import ValidationError as PydanticValidationError\n",
    "from pydantic_core import ValidationError as CoreValidationError\n",
    "\n",
    "# UTILITY IMPORTS\n",
    "from utils import ListOfEntities, PROMPT, validate_annotation_output_format, is_annotation_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26103257",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "llm = OpenAIChatModel(\n",
    "    \"meta-llama/llama-3-70b-instruct\",\n",
    "    provider=OpenRouterProvider(api_key=api_key)\n",
    ")\n",
    "clients = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40432668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAIChatModel()\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff87105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=llm,\n",
    "    output_type=ListOfEntities,\n",
    "    retries=3,\n",
    "    system_prompt=(\"Extract entities as structured JSON.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c95b37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 404, model_name: meta-llama/llama-3-70b-instruct, body: {'message': \"No endpoints found that support the provided 'tool_choice' value. To learn more about provider routing, visit: https://openrouter.ai/docs/provider-routing\", 'code': 404}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py:517\u001b[0m, in \u001b[0;36mOpenAIChatModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    516\u001b[0m     extra_headers\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m, get_user_agent())\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name,\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mopenai_messages,\n\u001b[1;32m    520\u001b[0m         parallel_tool_calls\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    521\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m OMIT,\n\u001b[1;32m    522\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice \u001b[38;5;129;01mor\u001b[39;00m OMIT,\n\u001b[1;32m    523\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    524\u001b[0m         stream_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_usage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m OMIT,\n\u001b[1;32m    525\u001b[0m         stop\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    526\u001b[0m         max_completion_tokens\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    527\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    528\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format \u001b[38;5;129;01mor\u001b[39;00m OMIT,\n\u001b[1;32m    529\u001b[0m         seed\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    530\u001b[0m         reasoning_effort\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_reasoning_effort\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    531\u001b[0m         user\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_user\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    532\u001b[0m         web_search_options\u001b[38;5;241m=\u001b[39mweb_search_options \u001b[38;5;129;01mor\u001b[39;00m OMIT,\n\u001b[1;32m    533\u001b[0m         service_tier\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_service_tier\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    534\u001b[0m         prediction\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    535\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    536\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    537\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    538\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    539\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    540\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_logprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    541\u001b[0m         top_logprobs\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_top_logprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, OMIT),\n\u001b[1;32m    542\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    543\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_body\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    544\u001b[0m     )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2584\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2587\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2588\u001b[0m         {\n\u001b[1;32m   2589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2590\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2591\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2594\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2596\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2597\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2599\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2600\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2601\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2602\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2603\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2604\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2605\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2606\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2607\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2608\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2609\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2612\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2615\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2619\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2620\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2621\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2622\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2623\u001b[0m         },\n\u001b[1;32m   2624\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2626\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2627\u001b[0m     ),\n\u001b[1;32m   2628\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2629\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2630\u001b[0m     ),\n\u001b[1;32m   2631\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2632\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2633\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2634\u001b[0m )\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1791\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1792\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1793\u001b[0m )\n\u001b[0;32m-> 1794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': \"No endpoints found that support the provided 'tool_choice' value. To learn more about provider routing, visit: https://openrouter.ai/docs/provider-routing\", 'code': 404}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModelHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROMPT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe text to annotate:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m      4\u001b[0m     result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/agent/abstract.py:225\u001b[0m, in \u001b[0;36mAbstractAgent.run\u001b[0;34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, instructions, deps, model_settings, usage_limits, usage, infer_name, toolsets, builtin_tools, event_stream_handler)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_name(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[1;32m    223\u001b[0m event_stream_handler \u001b[38;5;241m=\u001b[39m event_stream_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_stream_handler\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(\n\u001b[1;32m    226\u001b[0m     user_prompt\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m    227\u001b[0m     output_type\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[1;32m    228\u001b[0m     message_history\u001b[38;5;241m=\u001b[39mmessage_history,\n\u001b[1;32m    229\u001b[0m     deferred_tool_results\u001b[38;5;241m=\u001b[39mdeferred_tool_results,\n\u001b[1;32m    230\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    231\u001b[0m     instructions\u001b[38;5;241m=\u001b[39minstructions,\n\u001b[1;32m    232\u001b[0m     deps\u001b[38;5;241m=\u001b[39mdeps,\n\u001b[1;32m    233\u001b[0m     model_settings\u001b[38;5;241m=\u001b[39mmodel_settings,\n\u001b[1;32m    234\u001b[0m     usage_limits\u001b[38;5;241m=\u001b[39musage_limits,\n\u001b[1;32m    235\u001b[0m     usage\u001b[38;5;241m=\u001b[39musage,\n\u001b[1;32m    236\u001b[0m     toolsets\u001b[38;5;241m=\u001b[39mtoolsets,\n\u001b[1;32m    237\u001b[0m     builtin_tools\u001b[38;5;241m=\u001b[39mbuiltin_tools,\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_call_tools_node(node)\n\u001b[1;32m    242\u001b[0m         ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.13/contextlib.py:235\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    233\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mathrow(value)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/agent/__init__.py:649\u001b[0m, in \u001b[0;36mAgent.iter\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    638\u001b[0m run_span \u001b[38;5;241m=\u001b[39m tracer\u001b[38;5;241m.\u001b[39mstart_span(\n\u001b[1;32m    639\u001b[0m     instrumentation_names\u001b[38;5;241m.\u001b[39mget_agent_run_span_name(agent_name),\n\u001b[1;32m    640\u001b[0m     attributes\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m     },\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39miter(\n\u001b[1;32m    650\u001b[0m         inputs\u001b[38;5;241m=\u001b[39muser_prompt_node,\n\u001b[1;32m    651\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m    652\u001b[0m         deps\u001b[38;5;241m=\u001b[39mgraph_deps,\n\u001b[1;32m    653\u001b[0m         span\u001b[38;5;241m=\u001b[39muse_span(run_span) \u001b[38;5;28;01mif\u001b[39;00m run_span\u001b[38;5;241m.\u001b[39mis_recording() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    654\u001b[0m         infer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    655\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m graph_run:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m toolset:\n\u001b[1;32m    657\u001b[0m             agent_run \u001b[38;5;241m=\u001b[39m AgentRun(graph_run)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.13/contextlib.py:235\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    233\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mathrow(value)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_graph/beta/graph.py:270\u001b[0m, in \u001b[0;36mGraph.iter\u001b[0;34m(self, state, deps, inputs, span, infer_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m     entered_span \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(span)\n\u001b[1;32m    269\u001b[0m traceparent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m entered_span \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_traceparent(entered_span)\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m GraphRun[StateT, DepsT, OutputT](\n\u001b[1;32m    271\u001b[0m     graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    272\u001b[0m     state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m    273\u001b[0m     deps\u001b[38;5;241m=\u001b[39mdeps,\n\u001b[1;32m    274\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    275\u001b[0m     traceparent\u001b[38;5;241m=\u001b[39mtraceparent,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m graph_run:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m graph_run\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_graph/beta/graph.py:423\u001b[0m, in \u001b[0;36mGraphRun.__aexit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aexit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_val: Any, exc_tb: Any):\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_exit_stack\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m(exc_type, exc_val, exc_tb)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.13/contextlib.py:768\u001b[0m, in \u001b[0;36mAsyncExitStack.__aexit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.13/contextlib.py:749\u001b[0m, in \u001b[0;36mAsyncExitStack.__aexit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    747\u001b[0m     exc_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exc), exc, exc\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sync:\n\u001b[0;32m--> 749\u001b[0m     cb_suppress \u001b[38;5;241m=\u001b[39m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     cb_suppress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m cb(\u001b[38;5;241m*\u001b[39mexc_details)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.13/contextlib.py:162\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    160\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_graph/beta/graph.py:978\u001b[0m, in \u001b[0;36m_unwrap_exception_groups\u001b[0;34m()\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# bizarrely, this prevents recursion errors when formatting the exception for logfire\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_graph/beta/graph.py:750\u001b[0m, in \u001b[0;36m_GraphIterator._run_tracked_task\u001b[0;34m(self, t_)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m CancelScope() \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_scopes[t_\u001b[38;5;241m.\u001b[39mtask_id] \u001b[38;5;241m=\u001b[39m scope\n\u001b[0;32m--> 750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_task(t_)\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, _GraphTaskAsyncIterable):\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m new_tasks \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39miterable:\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_graph/beta/graph.py:779\u001b[0m, in \u001b[0;36m_GraphIterator._run_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    776\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(logfire_span(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun node \u001b[39m\u001b[38;5;132;01m{node_id}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, node_id\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39mid, node\u001b[38;5;241m=\u001b[39mnode))\n\u001b[1;32m    778\u001b[0m     step_context \u001b[38;5;241m=\u001b[39m StepContext[StateT, DepsT, Any](state\u001b[38;5;241m=\u001b[39mstate, deps\u001b[38;5;241m=\u001b[39mdeps, inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 779\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mcall(step_context)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NodeStep):\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_node(output, fork_stack)\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_graph/beta/step.py:253\u001b[0m, in \u001b[0;36mNodeStep._call_node\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    252\u001b[0m node \u001b[38;5;241m=\u001b[39m cast(BaseNode[StateT, DepsT, Any], node)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mrun(GraphRunContext(state\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mstate, deps\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdeps))\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:424\u001b[0m, in \u001b[0;36mModelRequestNode.run\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_did_stream:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mAgentRunError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must finish streaming before calling run()\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(ctx)\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:466\u001b[0m, in \u001b[0;36mModelRequestNode._make_request\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    465\u001b[0m model_settings, model_request_parameters, message_history, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(ctx)\n\u001b[0;32m--> 466\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrequest(message_history, model_settings, model_request_parameters)\n\u001b[1;32m    467\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mrequests \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish_handling(ctx, model_response)\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py:435\u001b[0m, in \u001b[0;36mOpenAIChatModel.request\u001b[0;34m(self, messages, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    430\u001b[0m check_allow_model_requests()\n\u001b[1;32m    431\u001b[0m model_settings, model_request_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m    432\u001b[0m     model_settings,\n\u001b[1;32m    433\u001b[0m     model_request_parameters,\n\u001b[1;32m    434\u001b[0m )\n\u001b[0;32m--> 435\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completions_create(\n\u001b[1;32m    436\u001b[0m     messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(OpenAIChatModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[1;32m    437\u001b[0m )\n\u001b[1;32m    438\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_response\n",
      "File \u001b[0;32m~/Bureau/mdner_llm/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py:547\u001b[0m, in \u001b[0;36mOpenAIChatModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (status_code \u001b[38;5;241m:=\u001b[39m e\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m--> 547\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, body\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mbody) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mModelHTTPError\u001b[0m: status_code: 404, model_name: meta-llama/llama-3-70b-instruct, body: {'message': \"No endpoints found that support the provided 'tool_choice' value. To learn more about provider routing, visit: https://openrouter.ai/docs/provider-routing\", 'code': 404}"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = await agent.run(f\"{PROMPT}\\nThe text to annotate:\\n\")\n",
    "    print(response)\n",
    "    result = response.output\n",
    "    print(result)\n",
    "\n",
    "except (PydanticValidationError, CoreValidationError, UnexpectedModelBehavior) as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "946c45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output=ListOfEntities(entities=[Molecule(label='MOL', text='T cell receptor'), Molecule(label='MOL', text='T Cell Receptor'), Molecule(label='MOL', text='TCR'), Molecule(label='MOL', text='peptide-major histocompatibility complex'), Molecule(label='MOL', text='pMHC'), Molecule(label='MOL', text='TCRs'), Molecule(label='SOFTNAME', text='Modeller'), Molecule(label='SOFTNAME', text='ColabFold')]))\n",
      "entities=[Molecule(label='MOL', text='T cell receptor'), Molecule(label='MOL', text='T Cell Receptor'), Molecule(label='MOL', text='TCR'), Molecule(label='MOL', text='peptide-major histocompatibility complex'), Molecule(label='MOL', text='pMHC'), Molecule(label='MOL', text='TCRs'), Molecule(label='SOFTNAME', text='Modeller'), Molecule(label='SOFTNAME', text='ColabFold')]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    model=llm,\n",
    "    output_type=ListOfEntities,\n",
    "    retries=3,\n",
    "    system_prompt=(\"Extract entities as structured JSON.\"),\n",
    ")\n",
    "try:\n",
    "    response = await agent.run(f\"{PROMPT}\\nThe text to annotate:\\n\")\n",
    "    print(response)\n",
    "    result = response.output\n",
    "    print(result)\n",
    "\n",
    "except (PydanticValidationError, CoreValidationError, UnexpectedModelBehavior) as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d37b1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_TO_ANNOTATE = \"\"\"Improved Coarse-Grained Modeling of Cholesterol-Containing Lipid Bilayers\n",
    "Cholesterol trafficking, which is an essential function in mammalian cells, is intimately connected to molecular-scale interactions through cholesterol modulation of membrane structure and dynamics and interaction with membrane receptors. Since these effects of cholesterol occur on micro- to millisecond time scales, it is essential to develop accurate coarse-grained simulation models that can reach these time scales. Cholesterol has been shown experimentally to thicken the membrane and increase phospholipid tail order between 0 and 40% cholesterol, above which these effects plateau or slightly decrease. Here, we showed that the published MARTINI coarse-grained force-field for phospholipid (POPC) and cholesterol fails to capture these effects. Using reference atomistic simulations, we systematically modified POPC and cholesterol bonded parameters in MARTINI to improve its performance. We showed that the corrections to pseudobond angles between glycerol and the lipid tails and around the oleoyl double bond particle (the angle-corrected model ) slightly improves the agreement of MARTINI with experimentally measured thermal, elastic, and dynamic properties of POPC membranes. The angle-corrected model improves prediction of the thickening and ordering effects up to 40% cholesterol but overestimates these effects at higher cholesterol concentration. In accordance with prior work that showed the cholesterol rough face methyl groups are important for limiting cholesterol self-association, we revised the coarse-grained representation of these methyl groups to better match cholesterol-cholesterol radial distribution functions from atomistic simulations. In addition, by using a finer-grained representation of the branched cholesterol tail than MARTINI, we improved predictions of lipid tail order and bilayer thickness across a wide range of concentrations. Finally, transferability testing shows that a model incorporating our revised parameters into DOPC outperforms other CG models in a DOPC/cholesterol simulation series, which further argues for its efficacy and generalizability. These results argue for the importance of systematic optimization for coarse-graining biologically important molecules like cholesterol with complicated molecular structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dde9af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output=ListOfEntities(entities=[Molecule(label='MOL', text='cholesterol'), Molecule(label='MOL', text='POPC'), Molecule(label='MOL', text='glycerol'), Molecule(label='MOL', text='DOPC'), Molecule(label='FFM', text='MARTINI')]))\n",
      "entities=[Molecule(label='MOL', text='cholesterol'), Molecule(label='MOL', text='POPC'), Molecule(label='MOL', text='glycerol'), Molecule(label='MOL', text='DOPC'), Molecule(label='FFM', text='MARTINI')]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = await agent.run(f\"{PROMPT}\\nThe text to annotate:\\n {TEXT_TO_ANNOTATE}\")\n",
    "    print(response)\n",
    "    result = response.output\n",
    "    print(result)\n",
    "\n",
    "except (PydanticValidationError, CoreValidationError, UnexpectedModelBehavior) as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c11ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdner-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
