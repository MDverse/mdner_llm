"""
Evaluate and benchmark annotations produced by several LLMs on molecular-dynamics texts.

This script loads JSON annotation files generated by LLMs (e.g., GPT-4, Meta Llama,
Qwen, MoonshotAI Kimik2) and evaluates their quality against expert-validated
annotations. It supports multiple frameworks (Instructor, LlamaIndex, PydanticAI)
and models hosted on OpenRouter (https://openrouter.ai/models).


Evaluation includes:

1. **JSON format validity**
   The response must match the expected schema:
   `{"entities": [ {"label": <str>, "text": <str>, ...}, ... ]}`
   (and optionally character positions when using `json_with_positions`).

2. **Hallucination detection**
  Each extracted entity must correspond to text actually present in the source document.

3. **Annotation correctness**
   Compares LLM-predicted entities with ground-truth expert annotations,
   computing metris like True Positive, False Positive, False Negative,
   Precision, Recall, F1, F_beta_0.5.


Outputs:
--------
- Per-annotation **Parquet files** with detailed metrics for each model and framework.
- Aggregated **Excel (.xlsx) summary** combining results across models and frameworks,
reporting format adherence, hallucination rate, and annotation accuracy.


Requirements:
-------------
- Annotation files must exist in `--annotations-dir`. These are typically generated
by running `extract_entities_all_texts.py` for several models/frameworks.


Usage:
=======
    uv run src/evaluate_json_annotations.py [--annotations-dir PATH]
                                            [--results-dir PATH]

Arguments:
==========
    --annotations-dir: PATH
        Directory containing the annotation JSON files to evaluate.
        Default: "results/llm_annotations"

    --results-dir: PATH
        Directory where all evaluation outputs (Parquet files+summary Excel) are saved.
        Default: "results/json_evaluation_stats/"

Example:
========
    uv run src/evaluate_json_annotations.py

This command evaluates all LLM-generated JSON annotations in `results/llm_annotations`,
computes per-annotation metrics, and saves results into a Parquet file,
e.g.,`results/json_evaluation_stats/per_text_metrics_2026-01-12T15-30-00.parquet`.
It then aggregates statistics by framework and model into a single Excel summary file,
e.g.,`results/json_evaluation_stats/evaluation_summary_2026-01-12T15-30-00.xlsx`.
"""

# METADATAS
__authors__ = ("Pierre Poulain", "Essmay Touami")
__contact__ = "pierre.poulain@u-paris.fr"
__copyright__ = "AGPL-3.0 license"
__date__ = "2025"
__version__ = "1.0.0"


# LIBRARY IMPORTS
import json
import re
import sys
import time
import unicodedata
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import numpy as np

sys.path.append(str(Path(__file__).resolve().parent.parent))

import click
import pandas as pd
from loguru import logger
from openai.types.chat import ChatCompletion
from pydantic import ValidationError as PydanticValidationError

# UTILITY IMPORTS
from models.pydantic_output_models import ListOfEntities, ListOfEntitiesPositions


# FUNCTIONS
def setup_logger(loguru_logger: Any, log_dir: str | Path = "logs") -> None:
    """Configure a Loguru logger to write logs into a rotating daily log file.

    Parameters
    ----------
    loguru_logger : Any
        A Loguru logger instance (typically `loguru.logger`).
    log_dir : str or Path, optional
        Directory where log files will be stored. Default is "logs".
    """
    # Ensure log directory exists
    log_folder = Path(log_dir)
    log_folder.mkdir(parents=True, exist_ok=True)
    # Reset any previous configuration
    loguru_logger.remove()
    log_file = (log_folder /
                f"evaluate_json_annotations_{datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}.log")

    # Define log format
    fmt = (
        "{time:YYYY-MM-DD HH:mm:ss}"
        "| <level>{level:<8}</level> "
        "| <level>{message}</level>"
    )
    loguru_logger.add(
        log_file,
        format=fmt,
        level="DEBUG",
        mode="w",
    )
    loguru_logger.add(
        sys.stdout,
        format=fmt,
        level="DEBUG",
    )


def ensure_dir(ctx, param, value: Path) -> Path:
    """
    Create the directory if it does not already exist.

    Callback for Click options to ensure the provided path
    is a valid directory. Behaves like `mkdir -p`.

    Parameters
    ----------
    ctx : click.Context
        The Click context for the current command invocation.
        (Required by Click callbacks but unused in this function.)
    param : click.Parameter
        The Click parameter associated with this callback.
        (Required by Click callbacks but unused in this function.)
    value : Path
        The directory path provided by the user, already converted
        into a `pathlib.Path` object by Click.

    Returns
    -------
    Path
        The same path, after ensuring the directory exists.
    """
    value.mkdir(parents=True, exist_ok=True)
    return value


def load_json_annotations_as_dataframe(annotations_dir: Path) -> pd.DataFrame:
    """
    Load JSON annotation files into a DataFrame.

    Each row corresponds to one JSON file, and each column corresponds
    to a top-level key found in the JSON objects.

    Parameters
    ----------
    annotations_dir : Path
        Directory containing JSON annotation files.

    Returns
    -------
    pd.DataFrame
        DataFrame with one row per file and JSON keys as columns.
    """
    logger.info(f"Loading annotations from {annotations_dir}...")
    records: list[dict[str, object]] = []

    # Iterate over all JSON files in the directory in sorted order
    for json_file in sorted(annotations_dir.glob("*.json")):
        try:
            # Open and parse the JSON file
            with json_file.open(encoding="utf-8") as handle:
                data: dict[str, object] = json.load(handle)
        except json.JSONDecodeError as exc:
            logger.warning(
                f"Skipping invalid JSON file {json_file.name}: {exc}"
            )
            continue

        # Add the source filename as metadata
        data["__file__"] = json_file.name
        # Store the parsed JSON object
        records.append(data)

    # Convert the list of dictionaries into a DataFrame
    df = pd.DataFrame.from_records(records)
    logger.success(
        f"Loaded {df.shape[0]} annotation files into DataFrame successfully!\n"
    )

    return df


def add_raw_text_column(
    df: pd.DataFrame,
    text_file_col: str = "text_file",
) -> pd.DataFrame:
    """
    Extract 'raw_text' from JSON into a new column 'text_to_annotate' in the DataFrame.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing a column with paths to JSON files.
    text_file_col : str
        Name of the column containing paths to JSON files.

    Returns
    -------
    pd.DataFrame
        DataFrame with an additional column 'text_to_annotate' containing
        the raw text from each JSON file.
    """
    logger.info("Adding the text to annotate in the dataframe...")
    raw_texts: list[str] = []

    # Iterate over the JSON file paths stored in the specified column
    for _idx, json_path in enumerate(df[text_file_col]):
        # Convert the path string to a Path object
        path = Path(json_path)

        # Handle missing files by appending an empty string
        if not path.exists():
            logger.warning(f"JSON file not found: {json_path}")
            raw_texts.append("")
            continue

        try:
            # Open and parse the JSON file
            with path.open(encoding="utf-8") as f:
                data = json.load(f)

            # Extract the 'raw_text' field, defaulting to an empty string if absent
            raw_text = data.get("raw_text", "")
        except (json.JSONDecodeError, OSError) as exc:
            # Handle unreadable or invalid JSON files
            logger.warning(f"Failed to read {json_path}: {exc}")
            raw_text = ""

        # Collect the extracted text in order
        raw_texts.append(raw_text)

    df = df.copy()
    df["text_to_annotate"] = raw_texts
    logger.success(
        f"Added 'text_to_annotate' column for {len(df)} files successfully!\n"
    )
    return df


def parse_model(
    obj: ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str | dict,
    model_class: type[ListOfEntities | ListOfEntitiesPositions],
) -> ListOfEntities | ListOfEntitiesPositions | None:
    """
    Parse an object into a Pydantic model instance containing `.entities`.

    Supports:
    - Direct Pydantic model instance
    - ChatCompletion object
    - JSON string
    - Python dict representing JSON

    Parameters
    ----------
    obj : ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str | dict
        The object to parse into a Pydantic model.
    model_class : type[ListOfEntities | ListOfEntitiesPositions]
        The Pydantic model class to use for parsing.

    Returns
    -------
    ListOfEntities | ListOfEntitiesPositions | None
        The parsed model instance with an `.entities` attribute, or `None`
        if parsing fails or if the result has no `.entities`.
    """
    try:
        # If the object is already an instance of the target model, reuse it
        if isinstance(obj, model_class):
            entities_model = obj

        # If the object is a ChatCompletion
        elif isinstance(obj, ChatCompletion):
            # Extract the message content
            content = obj.choices[0].message.content
            # And validate it as JSON against the Pydantic model
            entities_model = model_class.model_validate_json(content)

        # Otherwise, treat the input as JSON data (string or dict)
        else:
            json_str = obj if isinstance(obj, str) else json.dumps(obj)
            entities_model = model_class.model_validate_json(json_str)

    # Catch validation, type, or JSON-related errors
    except (PydanticValidationError, ValueError, TypeError):
        return None

    # Ensure the parsed model exposes an `entities` attribute
    if not hasattr(entities_model, "entities"):
        return None

    return entities_model


def _safe_div(num: float, den: float) -> float:
    return num / den if den > 0 else 0.0


def compute_confusion_metrics(
    df: pd.DataFrame,
    results_dir: Path,
    pred_col: str = "raw_llm_response",
    gt_col: str = "groundtruth",
    text_col: str = "text_to_annotate",
    prompt_tag_col: str = "tag_prompt",
    beta: float = 0.5
) -> pd.DataFrame:
    """
    Compute confusion matrix metrics per annotation file.

    Metrics are computed at entity level using exact matching
    on tuples (label, normalized_text).

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame containing predictions, groundtruth, original text,
        and prompt tag.
    results_dir : Path
        Directory where the Excel file will be saved.
    pred_col : str
        Column name containing LLM responses.
    gt_col : str
        Column name containing ground-truth annotations.
    text_col : str
        Column name containing the original text that was annotated.
    prompt_tag_col : str
        Column name defining expected JSON format.
    beta : float
        Beta value for F-beta score.

    Returns
    -------
    pd.DataFrame
        DataFrame with confusion metrics and quality flags added.
    """
    logger.info("Computing evaluation metrics per annotation...")

    def _compute_row(row: pd.Series) -> pd.Series:
        # Extract row info
        response = row[pred_col]
        groundtruth = row[gt_col]
        original_text = row[text_col]
        prompt_tag = row[prompt_tag_col]
        file = row["__file__"]

        # Check format and hallucinations
        is_format_valid = is_valid_output_format(response, prompt_tag)
        no_hallucination = has_no_hallucination(
            response,
            original_text,
            prompt_tag,
        )

        # Determine model type based on prompt
        model_class = (
            ListOfEntities if prompt_tag == "json" else ListOfEntitiesPositions
        )
        # Parse prediction and groundtruth
        pred_model = parse_model(response, model_class)
        gt_model = parse_model(groundtruth, model_class)

        if pred_model is None or gt_model is None:
            # Skip metrics if parsing fails
            reason = (
                "prediction parsing failed."
                if pred_model is None
                else "groundtruth parsing failed."
            )
            logger.warning(f"[{file}] Metrics skipped: {reason}")

            return pd.Series(
                {
                    "is_format_valid": is_format_valid,
                    "has_no_hallucination": no_hallucination,
                    "true_positives": 0,
                    "false_positives": 0,
                    "false_negatives": 0,
                    "precision_of_annotation": 0.0,
                    "recall_of_annotation": 0.0,
                    "f1_of_annotation": 0.0,
                    f"fbeta_{beta}_of_annotation": 0.0,
                }
            )
        # Convert entities to sets of (label, normalized_text)
        preds_set = {(e.label, normalize_text(e.text)) for e in pred_model.entities}
        gt_set = {(e.label, normalize_text(e.text)) for e in gt_model.entities}

        # Compute confusion counts
        tp = len(preds_set & gt_set)
        fp = len(preds_set - gt_set)
        fn = len(gt_set - preds_set)
        # Compute metrics safely
        precision = _safe_div(tp, tp + fp)
        recall = _safe_div(tp, tp + fn)
        f1 = _safe_div(2 * precision * recall, precision + recall)
        fbeta = _safe_div(
            (1 + beta**2) * precision * recall,
            (beta**2 * precision) + recall,
        )

        logger.info(f"[{file}]:")
        logger.info(f"Is correct output format: {is_format_valid}")
        logger.info(f"Is without hallucination: {no_hallucination}")
        logger.debug(f"Predictions ({len(preds_set)}): {sorted(preds_set)}")
        logger.debug(f"Groundtruth ({len(gt_set)}): {sorted(gt_set)}")
        logger.info(f"Confusion counts: TP={tp} | FP={fp} | FN={fn}")
        logger.info(
            f"Evaluation metrics: precision={precision:.3f} | "
            f"recall={recall:.3f} | f1={f1:.3f} | "
            f"fbeta_{beta}={fbeta:.3f}\n"
        )

        return pd.Series(
            {
                "is_format_valid": is_format_valid,
                "has_no_hallucination": no_hallucination,
                "true_positives": tp,
                "false_positives": fp,
                "false_negatives": fn,
                "precision_of_annotation": precision,
                "recall_of_annotation": recall,
                "f1_of_annotation": f1,
                f"fbeta_{beta}_of_annotation": fbeta,
            }
        )

    # Apply row-wise computation
    metrics = df.apply(_compute_row, axis=1)
    res_df = pd.concat([df, metrics], axis=1)

    # Saving metrics by text into a parquet file
    parquet_path = (results_dir /
                    f"per_text_metrics_{datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}.parquet")
    res_df.to_parquet(parquet_path, index=False)

    logger.success(
        f"Saved metrics computation for each files in {parquet_path} successfully!\n")
    return res_df


def _extract_json_string(
    response: ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str
) -> str | None:
    """
    Extract the JSON string from a model response.

    Parameters
    ----------
    response : ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str
        The raw response from the model. It can be:
        - a Pydantic model (`ListOfEntities` or `ListOfEntitiesPositions`),
        - a `ChatCompletion` object containing JSON text,
        - or a raw JSON string.

    Returns
    -------
    str | None
        The extracted JSON string if available, otherwise `None`.
    """
    # If response is already a string, return it
    if isinstance(response, str):
        return response

    # If response is a ChatCompletion object
    if isinstance(response, ChatCompletion):
        choices = getattr(response, "choices", None)
        if not choices:  # no choices available
            return None

        message = getattr(choices[0], "message", None)
        if message is None:  # message missing
            return None

        # Return the content of the first message
        return getattr(message, "content", None)

    # For other types (e.g., Pydantic models), return None
    return None


def _validate_json_string(response_str: str, prompt_tag: str) -> bool:
    """
    Validate a JSON string against the expected Pydantic model.

    Parameters
    ----------
    response_str : str
        JSON string to validate.
    prompt_tag : str
        Tag specifying the expected JSON format:
        - "json" for `ListOfEntities`
        - "json_with_positions" for `ListOfEntitiesPositions`.

    Returns
    -------
    bool
        True if the JSON string is valid according to the expected format,
        False otherwise.
    """
    try:
        # Validate JSON string against the appropriate Pydantic model
        if prompt_tag == "json":
            ListOfEntities.model_validate_json(response_str)
        else:
            ListOfEntitiesPositions.model_validate_json(response_str)
        return True  # Validation succeeded
    except PydanticValidationError:
        return False  # Validation failed


def is_valid_output_format(
    response: ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str,
    prompt_tag: str
) -> bool:
    """
    Check whether the model response is valid according to the expected output format.

    Parameters
    ----------
    response : Any
        The raw model response:
        - a Pydantic model (ListOfEntities or ListOfEntitiesPositions),
        - a ChatCompletion object containing JSON text,
        - or a raw JSON string.
    prompt_tag : str
        Tag defining expected JSON format ('json' or 'json_with_positions').

    Returns
    -------
    bool
        True if the response is valid, False otherwise.
    """
    # Case 1: Already a Pydantic instance
    if (isinstance(response, ListOfEntities) and prompt_tag == "json") or \
       (isinstance(response, ListOfEntitiesPositions)
        and prompt_tag == "json_with_positions"):
        return True

    # Case 2: Extract JSON string
    response_str = _extract_json_string(response)
    if response_str is None:
        return False

    return _validate_json_string(response_str, prompt_tag)


def has_no_hallucination(
    response: ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str,
    original_text: str,
    prompt_tag: str = "json"
) -> bool:
    """
    Check that all predicted entities appear in the original text.

    Parameters
    ----------
    response : ListOfEntities | ListOfEntitiesPositions | ChatCompletion | str
        The validated model response or raw JSON string.
    original_text : str
        The text that was annotated.
    prompt_tag : str
        Tag defining expected JSON format ("json" or "json_with_positions").

    Returns
    -------
    bool
        True if no predicted entity is missing from the original text.
    """
    if not is_valid_output_format(response, prompt_tag):
        return False
    # Select model class
    model_class = ListOfEntities if prompt_tag == "json" else ListOfEntitiesPositions

    # Parse response into entities_model
    try:
        if isinstance(response, model_class):
            entities_model = response
        elif isinstance(response, ChatCompletion):
            content = response.choices[0].message.content
            entities_model = model_class.model_validate_json(content)
        else:
            entities_model = model_class.model_validate_json(response)
    except PydanticValidationError:
        return False

    # Ensure entity list exists
    if not hasattr(entities_model, "entities"):
        return False

    # Normalize text once
    norm_text = normalize_text(original_text)

    # Check each entity actually appears in original text
    for entity in entities_model.entities:
        if not entity.text:
            return False
        if normalize_text(entity.text) not in norm_text:
            return False

    return True


def safe_divide(num: pd.Series, den: pd.Series) -> pd.Series:
    """
    Perform element-wise division with explicit zero-denominator handling.

    Parameters
    ----------
    num : pd.Series
        Numerator values.
    den : pd.Series
        Denominator values.

    Returns
    -------
    pd.Series
        Result of num / den where den > 0, otherwise 0.0,
        rounded to 3 decimal places.
    """
    # Divide where denominator > 0, else set result to 0
    result = np.where(den > 0, num / den, 0.0)
    # Round the results to 3 decimal places
    return np.round(result, 3)


def compute_grouped_stats(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute evaluation statistics grouped by framework and model.

    For each combination of `framework_name` and `model_name`, this function:
    - Computes the number of annotations
    - Aggregates true positives, false positives, and false negatives
    - Computes precision, recall, F1 score, and F-beta score (Î²=0.5 by default)

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing at least the following columns:
        - "framework_name": name of the annotation framework
        - "model_name": LLM model name
        - "raw_llm_response": raw LLM output
        - "text_to_annotate": original text that was annotated
        - "prompt_tag": tag specifying expected JSON format
            ("json" or "json_with_positions")
        - "true_positives", "false_positives", "false_negatives": counts per row

    Returns
    -------
    pd.DataFrame
        Aggregated metrics per framework and model with the following columns:
        - "framework_name", "model_name"
        - "nb_annotations": number of annotations
        - "pct_is_format_valid": fraction of valid format responses
        - "pct_without_hallucination": fraction of responses without hallucinations
        - "true_positives", "false_positives", "false_negatives": summed per group
        - "precision", "recall", "f1", "fbeta_0.5": aggregated metrics
    """
    logger.info("Computing evaluation metrics grouped by framework and models...")
    df = df.copy()

    # Aggregate per framework and per model
    grouped = (
        df.groupby(["framework_name", "model_name", "tag_prompt"])
        .agg(
            nb_annotations=("raw_llm_response", "count"),
            pct_is_format_valid=("is_format_valid", lambda s: 100 * s.mean()),
            pct_has_without_hallucination=(
                "has_no_hallucination", lambda s: 100 * s.mean()),
            true_positives=("true_positives", "sum"),
            false_positives=("false_positives", "sum"),
            false_negatives=("false_negatives", "sum"),
        )
        .reset_index()
    )

    # Compute precision, recall, F1, F-beta per group
    beta = 0.5
    tp = grouped["true_positives"]
    fp = grouped["false_positives"]
    fn = grouped["false_negatives"]

    grouped["precision"] = safe_divide(tp, tp + fp)
    grouped["recall"] = safe_divide(tp, tp + fn)

    grouped["f1"] = safe_divide(
        2 * grouped["precision"] * grouped["recall"],
        grouped["precision"] + grouped["recall"],
    )

    grouped[f"fbeta_{beta}"] = safe_divide(
        (1 + beta**2) * grouped["precision"] * grouped["recall"],
        beta**2 * grouped["precision"] + grouped["recall"],
    )

    return grouped


def normalize_text(text: str) -> str:
    """Normalize text by removing special characters and converting to lowercase.

    Parameters
    ----------
    text : str
        The text to normalize.

    Returns
    -------
    str
        The normalized text.
    """
    # Normalize unicode characters
    text_normalized = unicodedata.normalize("NFKD", text)
    # Convert to lowercase
    text_normalized = text_normalized.lower()
    # Remove extra whitespace
    text_normalized = re.sub(r"\s+", " ", text_normalized)
    # Strip leading and trailing whitespace
    text_normalized = text_normalized.strip()
    return text_normalized


def serialize_response(resp: Any) -> str:
    """
    Serialize various response objects into a JSON-safe string representation.

    Parameters
    ----------
    resp : Any
        The object to serialize. This may be a string, a custom class instance,
        or a model response object such as ChatCompletion.

    Returns
    -------
    str
        A JSON-compatible string representation of the input object.
    """
    # If it's already a string, nothing to do.
    if isinstance(resp, str):
        return resp

    # If it's a ListOfEntities or ListOfEntitiesPositions object
    if isinstance(resp, (ListOfEntities, ListOfEntitiesPositions)):
        return resp.model_dump_json(indent=2)

    # Specific handling for ChatCompletion-like objects
    if isinstance(resp, ChatCompletion):
        return json.dumps(resp.__dict__, default=str)

    return str(resp)


def save_grouped_stats_to_excel(
    df: pd.DataFrame,
    results_dir: Path,
    filename_prefix: str = "evaluation_summary",
) -> Path:
    """
    Save grouped evaluation statistics to an Excel file with MultiIndex columns.

    The output Excel file is structured with one row per model and
    MultiIndex columns of the form:
        (framework_name, metric)

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame produced by `compute_grouped_stats`.
    results_dir : Path
        Directory where the Excel file will be saved.
    filename_prefix : str
        Prefix used for the output Excel filename.
        Default: evaluation_summary.

    Returns
    -------
    Path
        Path to the generated Excel file.
    """
    metrics_map = {
        "nb_annotations": "Number of Annotations",
        "pct_is_format_valid": "Is correct Output Format",
        "pct_has_without_hallucination": "Has Without Hallucination",
        "precision": "Precision",
        "recall": "Recall",
        "f1": "F1 Score",
        "fbeta_0.5": "F-beta 0.5",
    }
    metrics = list(metrics_map.keys())

    # Pivot to include framework in columns
    df_pivot = df.pivot_table(
        index="model_name",
        columns="framework_name",
        values=metrics,
        aggfunc="first",  # or "mean"
    )

    # Rename columns: metric -> label lisible, framework -> title case
    df_pivot.columns = pd.MultiIndex.from_tuples([
        (fw.replace("_", " ").title(), metrics_map[m])
        for m, fw in df_pivot.columns
    ])

    # Order metrics inside each framework
    ordered_tuples = [
        (fw, m)
        for fw in df_pivot.columns.get_level_values(0).unique()
        for m in metrics_map.values()
        if (fw, m) in df_pivot.columns
    ]
    df_pivot = df_pivot.loc[:, ordered_tuples]

    # Save Excel
    output_path = (results_dir /
        f"{filename_prefix}_{datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}.xlsx")
    df_pivot.to_excel(output_path, index=True)

    logger.success(
        f"Grouped evaluation statistics saved successfully to: {output_path}\n")

    return output_path


@click.command()
@click.option(
    "--annotations-dir",
    type=click.Path(exists=True, file_okay=False, dir_okay=True, path_type=Path),
    default=Path("results/llm_annotations"),
    show_default=True,
    help="Directory containing the JSON annotation files to evaluate."
)
@click.option(
    "--results-dir",
    type=click.Path(file_okay=False, dir_okay=True, path_type=Path),
    default=Path("results/annotations_evaluation_stats"),
    show_default=True,
    help="Target directory where evaluation results will be saved.",
    callback=ensure_dir
)
def evaluate_json_annotations(
    annotations_dir: Path,
    results_dir: Path,
) -> None:
    """
    Evaluate the quality of JSON entity annotations.

    Parameters
    ----------
    annotations_dir : Path
        Directory containing the JSON annotation files to evaluate.
    results_dir : Path
        Directory where evaluation results, logs, and reports will be written.
    """
    # Configure logging
    setup_logger(logger, results_dir)
    logger.info("Starting evaluation of JSON annotation outputs...")
    logger.info(f"Annotations directory: {annotations_dir}")
    logger.info(f"Results directory: {results_dir} \n")
    start_time = time.perf_counter()

    # Loading annotations with metadatas
    df = load_json_annotations_as_dataframe(annotations_dir)
    # Adding the text to annotate into the df
    df_with_text = add_raw_text_column(df)

    # Compute confusion metrics (TP, FP, TN) by text
    df_with_conf_metrics = compute_confusion_metrics(df_with_text, results_dir)

    # Compute confusion metrics (TP, FP, TN) grouped by framework and model
    df_grouped_stats = compute_grouped_stats(df_with_conf_metrics)

    # Saving into an excel
    save_grouped_stats_to_excel(df=df_grouped_stats, results_dir=results_dir)

    elapsed_time = int(time.perf_counter() - start_time)
    logger.success(
        f"Evaluation duration: {timedelta(seconds=elapsed_time)} ðŸŽ‰")


# MAIN PROGRAM
if __name__ == "__main__":
    # Evaluate json annotations through all models
    evaluate_json_annotations()
